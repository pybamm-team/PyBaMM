{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_S_config_total(x, S_config_params_list):\n",
    "    \"\"\"\n",
    "    S_total = -R(x*log(x) + (1-x)*log(1-x))*(1+\\sum_{i=0}^n \\Omega_i P_i(1-2x))\n",
    "    where P_i is i-th order Legendre/Chebyshev polynomial\n",
    "    S_config_params_list = [omega0, omega1, ... omega_n], length n+1, up to nth order\n",
    "    \"\"\"\n",
    "    S_ideal = -8.314 * (x * torch.log(x) + (1 - x) * torch.log(1 - x))\n",
    "    S_expand = 1.0\n",
    "    t = 1 - 2 * x\n",
    "    Pn_values = legendre_poly_recurrence(t, len(S_config_params_list) - 1)\n",
    "    for i in range(0, len(S_config_params_list)):\n",
    "        S_expand = S_expand + S_config_params_list[i] * Pn_values[i]\n",
    "    S_total = S_ideal * S_expand\n",
    "    return S_total, S_ideal, S_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _S_vib_single_Einstein_model(x, Theta_Li, T=320, Theta_Li_scaled_100_times=True):\n",
    "    \"\"\"\n",
    "    similar to derivation in https://pubs.acs.org/doi/10.1021/acs.jpcc.1c10414, Equation 20 & 21\n",
    "    For an Einstein model we have\n",
    "    S_vib = -3nk_B[log(1-exp(-Theta_E/T)) - Theta_E/T * 1/(exp(Theta_E/T) -1)]\n",
    "    x is the filling fraction\n",
    "    Theta_Li is the learnable parameter (or a list that contains the learnable params), might be scaled by 100 times\n",
    "    T is the temperature\n",
    "    style: if it is Theta_Li, what polynomial will be used\n",
    "    \"\"\"\n",
    "    if isinstance(Theta_Li, list) == True:\n",
    "        # this is a polynomial expansion\n",
    "        _t = 1 - 2 * x\n",
    "        Pn_values = legendre_poly_recurrence(_t, len(Theta_Li) - 1)\n",
    "        # calculate temperature\n",
    "        t = 0.0\n",
    "        # print(Theta_Li)\n",
    "        for i in range(0, len(Theta_Li)):\n",
    "            t = t + Theta_Li[i] * Pn_values[i]\n",
    "    else:\n",
    "        # it's a constant temperature\n",
    "        t = Theta_Li * torch.tensor(1.0)\n",
    "    if Theta_Li_scaled_100_times == True:\n",
    "        t = -(t * 100) / T  # remember Theta_Li initialized 100 times smaller!!\n",
    "    else:\n",
    "        t = -t / T\n",
    "    S_vib = -3 * 8.314 * (torch.log(1.0 - torch.exp(t)) + t * 1.0 / (torch.exp(-t) - 1))\n",
    "    return S_vib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_S_vib_total(\n",
    "    x, n_list, Theta_E_list, T=320, Theta_Li_scaled_100_times=True, style=\"Legendre\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Weighted average of several Einstein models, the number of Einstein models is len(n_list) = len(Theta_E_list)\n",
    "    x is the filling fraction\n",
    "    n_list is how many moles of atoms are there in 1 mole of substance, the first element is wrong (should be deduced from the other two elements)\n",
    "    Theta_E_list is the list of learnable effective Einstein temperature for each of the Einstein model\n",
    "    T is the temperature\n",
    "    \"\"\"\n",
    "    assert len(n_list) == len(Theta_E_list)\n",
    "    S_vib = 0.0\n",
    "    ## we have s_excess = s_LixHM - s_HM - x*s_Li\n",
    "    ## LixHM: there is 1 mole of HM, and x mole of Li\n",
    "    S_vib = S_vib + (1.0 * n_list[1] + x * n_list[2]) * _S_vib_single_Einstein_model(\n",
    "        x, Theta_E_list[0], T=T, Theta_Li_scaled_100_times=Theta_Li_scaled_100_times\n",
    "    )\n",
    "    ## HM: there is 1 mole of HM\n",
    "    S_vib = S_vib - (1.0 * n_list[1]) * _S_vib_single_Einstein_model(\n",
    "        x, Theta_E_list[1], T=T, Theta_Li_scaled_100_times=Theta_Li_scaled_100_times\n",
    "    )\n",
    "    ## Li: there is x mole of Li\n",
    "    S_vib = S_vib - (x * n_list[2]) * _S_vib_single_Einstein_model(\n",
    "        x, Theta_E_list[2], T=T, Theta_Li_scaled_100_times=Theta_Li_scaled_100_times\n",
    "    )\n",
    "    return S_vib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legendre_poly_recurrence(x, n):\n",
    "    \"\"\"\n",
    "    Compute the Legendre polynomials up to degree n\n",
    "    using the Bonnet's recursion formula (i+1)P_(i+1)(x) = (2i+1)xP_i(x) - iP_(i-1)(x)\n",
    "    and return all n functions in a list\n",
    "    \"\"\"\n",
    "    P = [torch.ones_like(x), x]  # P_0(x) = 1, P_1(x) = x\n",
    "    for i in range(1, n):\n",
    "        P_i_plus_one = ((2 * i + 1) * x * P[i] - i * P[i - 1]) / (i + 1)\n",
    "        P.append(P_i_plus_one)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legendre_derivative_poly_recurrence(x, n):\n",
    "    \"\"\"\n",
    "    Compute the Legendre polynomials derivatives up to degree n\n",
    "    using (x^2-1)/n P'n(x) = xP_n(x) - P_(n-1)(x),\n",
    "    and return all n functions in a list\n",
    "    \"\"\"\n",
    "    Pn_values = legendre_poly_recurrence(x, n)\n",
    "    Pn_derivatives = [0.0]\n",
    "    for i in range(1, n + 1):\n",
    "        Pn_derivative_next = (x * Pn_values[i] - Pn_values[i - 1]) / ((x**2 - 1) / i)\n",
    "        Pn_derivatives.append(Pn_derivative_next)\n",
    "    return Pn_derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read hysterisis data\n",
    "path = r\"C:\\UM\\Study_Material\\Capstone\\OCP_FIT\\Data\"\n",
    "loc = os.path.join(\n",
    "    path, \"TdS_dx_lithiation_320K_modified.csv\"\n",
    ")  # deleted those datapoints within miscibility gaps\n",
    "working_dir = os.getcwd()\n",
    "df = pd.read_csv(loc, header=None)\n",
    "data = df.to_numpy()\n",
    "x_measured = data[:, 0]  # Li filling fraction of graphite, from 0 to 1\n",
    "TdSdx = data[\n",
    "    :, 1\n",
    "]  # unit is eV/6C, measured at 320K -- 6C means 6 carbons, i.e. per formular\n",
    "dsdx_measured = TdSdx / 320 * 96485  # now eV*96485 = J/mol\n",
    "\n",
    "\n",
    "# convert to torch.tensor\n",
    "x_measured = x_measured.astype(\"float32\")\n",
    "x_measured = torch.from_numpy(x_measured)\n",
    "dsdx_measured = dsdx_measured.astype(\"float32\")\n",
    "dsdx_measured = torch.from_numpy(dsdx_measured)\n",
    "T = 320  # measured at 320\n",
    "\n",
    "with open(\"log_entropy\", \"w\") as fout:\n",
    "    fout.write(\"\")\n",
    "\n",
    "os.makedirs(\"records_entropy\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## omegas for excess configurational entropy\n",
    "## load a good guess\n",
    "omega0 = nn.Parameter(torch.from_numpy(np.array([-0.5600], dtype=\"float32\")))\n",
    "omega1 = nn.Parameter(torch.from_numpy(np.array([-0.1245], dtype=\"float32\")))\n",
    "omega2 = nn.Parameter(torch.from_numpy(np.array([0.3012], dtype=\"float32\")))\n",
    "omega3 = nn.Parameter(torch.from_numpy(np.array([-0.0237], dtype=\"float32\")))\n",
    "omega4 = nn.Parameter(torch.from_numpy(np.array([-0.5114], dtype=\"float32\")))\n",
    "S_config_params_list = [omega0, omega1, omega2, omega3, omega4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Theta_Es and As for Einstein models\n",
    "## Theta_Li is scaled by 100 time\n",
    "## initialize Theta_LiHM as a function of x\n",
    "# Theta_LiHM = []\n",
    "# Theta_LiHM_order_expansion = 8\n",
    "# coeff_now = nn.Parameter( torch.from_numpy(np.array([200.0/100],dtype=\"float32\")) )# Theta_Li is scaled by 100 time\n",
    "# Theta_LiHM.append(coeff_now)\n",
    "# for i in range(1, Theta_LiHM_order_expansion+1):\n",
    "#     coeff_now = nn.Parameter( torch.from_numpy(np.array([(np.random.random()*2-1)/100],dtype=\"float32\")) )# Theta_Li is scaled by 100 time\n",
    "#     Theta_LiHM.append(coeff_now)\n",
    "# ## load a good guess\n",
    "ThetaLiHM0 = nn.Parameter(torch.from_numpy(np.array([252.5322 / 100], dtype=\"float32\")))\n",
    "ThetaLiHM1 = nn.Parameter(torch.from_numpy(np.array([-26.5938 / 100], dtype=\"float32\")))\n",
    "ThetaLiHM2 = nn.Parameter(torch.from_numpy(np.array([-1.2728 / 100], dtype=\"float32\")))\n",
    "ThetaLiHM3 = nn.Parameter(torch.from_numpy(np.array([1.9412 / 100], dtype=\"float32\")))\n",
    "ThetaLiHM4 = nn.Parameter(torch.from_numpy(np.array([-0.3446 / 100], dtype=\"float32\")))\n",
    "ThetaLiHM5 = nn.Parameter(torch.from_numpy(np.array([-0.2208 / 100], dtype=\"float32\")))\n",
    "ThetaLiHM6 = nn.Parameter(torch.from_numpy(np.array([0.7924 / 100], dtype=\"float32\")))\n",
    "ThetaLiHM7 = nn.Parameter(torch.from_numpy(np.array([0.6615 / 100], dtype=\"float32\")))\n",
    "ThetaLiHM8 = nn.Parameter(torch.from_numpy(np.array([0.4697 / 100], dtype=\"float32\")))\n",
    "Theta_LiHM = [\n",
    "    ThetaLiHM0,\n",
    "    ThetaLiHM1,\n",
    "    ThetaLiHM2,\n",
    "    ThetaLiHM3,\n",
    "    ThetaLiHM4,\n",
    "    ThetaLiHM5,\n",
    "    ThetaLiHM6,\n",
    "    ThetaLiHM7,\n",
    "    ThetaLiHM8,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see how to convert Debye temperature into Einstein Temperature: https://en.wikipedia.org/wiki/Debye_model?utm_source=chatgpt.com#Debye_versus_Einstein\n",
    "## Li Debye temperature: https://www.sciencedirect.com/science/article/pii/S0378775303002854\n",
    "## graphite Debye temperature: https://journals.aps.org/prb/abstract/10.1103/PhysRevB.73.064304\n",
    "Theta_HM = nn.Parameter(\n",
    "    torch.from_numpy(np.array([402 * 0.805995977 / 100], dtype=\"float32\"))\n",
    ")  # Theta is scaled by 100 time\n",
    "Theta_Li = nn.Parameter(\n",
    "    torch.from_numpy(np.array([380 * 0.805995977 / 100], dtype=\"float32\"))\n",
    ")  # Theta is scaled by 100 time\n",
    "Theta_E_list = [Theta_LiHM, Theta_HM, Theta_Li]\n",
    "## how many moles of atoms are there in one mole of LixHM, HM and Li\n",
    "n_list = [\n",
    "    9999.9,\n",
    "    6.0,\n",
    "    1.0,\n",
    "]  # there are 6 moles of C in 1 mole of C6, and 1 mole of Li in 1 mole of lithium\n",
    "# the first element of n_list should be recalculated by 1*n_list[1] + x*n_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## declare all parameters\n",
    "params_list = []  # for optimizer\n",
    "for item in S_config_params_list:\n",
    "    params_list.append(item)\n",
    "# this is Theta_LiHM which is expanded as a polynomial\n",
    "for j in range(0, len(Theta_E_list[0])):\n",
    "    params_list.append(Theta_E_list[0][j])\n",
    "## we don't train Theta_HM and Theta_Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init optimizer\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.Adam(params_list, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0  Loss tot 22710.1406 dsdx 22710.1406 s>0 0.0000 s<s_max 0.0000    omega0 -0.4600 omega1 -0.0245 omega2 0.2012 omega3 -0.1237 omega4 -0.4114 ThetaLiHM0 262.5322 ThetaLiHM1 -16.5938 ThetaLiHM2 8.7272 ThetaLiHM3 11.9412 ThetaLiHM4 -10.3446 ThetaLiHM5 9.7792 ThetaLiHM6 -9.2076 ThetaLiHM7 -9.3385 ThetaLiHM8 10.4697 ThetaE1 324.0104 ThetaE2 306.2785       \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# train\n",
    "loss = 9999.9  # init total loss\n",
    "epoch = -1\n",
    "while loss > 0.01 and epoch < 20000:\n",
    "    # clean grad info\n",
    "    optimizer.zero_grad()\n",
    "    # use current params to calculate predicted phase boundary\n",
    "    epoch = epoch + 1\n",
    "    # init loss components\n",
    "    loss = 0.0  # init total loss\n",
    "    dsdx_calculated = torch.zeros(len(dsdx_measured))\n",
    "    for i in range(0, len(x_measured)):\n",
    "        x = x_measured[i]\n",
    "        x = x.requires_grad_()\n",
    "        s_config_tot, _, _ = calculate_S_config_total(x, S_config_params_list)\n",
    "        s_vib_tot = calculate_S_vib_total(x, n_list, Theta_E_list, T=T)\n",
    "        s_tot = s_config_tot + s_vib_tot\n",
    "        ds_dx = autograd.grad(outputs=s_tot, inputs=x, create_graph=True)[0]\n",
    "        dsdx_calculated[i] = ds_dx\n",
    "\n",
    "    x_calculated = np.linspace(0.0001, 0.9999, 100).astype(\"float32\")\n",
    "    x_calculated = torch.from_numpy(x_calculated)\n",
    "    s_calculated = torch.zeros(len(x_calculated))\n",
    "    s_upper_bound = torch.zeros(len(x_calculated))\n",
    "    for i in range(0, len(x_calculated)):\n",
    "        x = x_calculated[i]\n",
    "        x = x.requires_grad_()\n",
    "        s_tot, _, _ = calculate_S_config_total(x, S_config_params_list)\n",
    "        s_calculated[i] = s_tot\n",
    "        s_upper_bound_now, _, _ = calculate_S_config_total(x, [0.0])\n",
    "        s_upper_bound[i] = s_upper_bound_now\n",
    "\n",
    "    # s_config should be larger than 0\n",
    "    mask_s_lower_bound = (s_calculated <= 0).int()\n",
    "    loss_s_config_leq_0 = torch.sum((s_calculated * mask_s_lower_bound) ** 2) * 1000000\n",
    "    # s_config should be smaller than ideal configurational entropy\n",
    "    mask_s_upper_bound = (s_calculated >= s_upper_bound).int()\n",
    "    loss_s_config_geq_upper_bound = (\n",
    "        torch.sum(((s_calculated - s_upper_bound) * mask_s_upper_bound) ** 2) * 1000000\n",
    "    )\n",
    "    # minimize data loss\n",
    "    loss_dsdx = torch.sum((dsdx_calculated - dsdx_measured) ** 2)\n",
    "    # total loss\n",
    "    loss = loss_s_config_leq_0 + loss_s_config_geq_upper_bound + loss_dsdx\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print output\n",
    "    output_txt = \"Epoch %3d  Loss tot %.4f dsdx %.4f s>0 %.4f s<s_max %.4f    \" % (\n",
    "        epoch,\n",
    "        loss,\n",
    "        loss_dsdx,\n",
    "        loss_s_config_leq_0,\n",
    "        loss_s_config_geq_upper_bound,\n",
    "    )\n",
    "    for i in range(0, len(S_config_params_list)):\n",
    "        output_txt = output_txt + \"omega%d %.4f \" % (i, S_config_params_list[i].item())\n",
    "    for i in range(0, len(Theta_E_list[0])):\n",
    "        output_txt = output_txt + \"ThetaLiHM%d %.4f \" % (\n",
    "            i,\n",
    "            Theta_E_list[0][i].item() * 100,\n",
    "        )\n",
    "    for i in range(1, len(Theta_E_list)):\n",
    "        output_txt = output_txt + \"ThetaE%d %.4f \" % (i, Theta_E_list[i].item() * 100)\n",
    "    output_txt = output_txt + \"      \"\n",
    "    print(output_txt)\n",
    "    with open(\"log_entropy\", \"a\") as fout:\n",
    "        fout.write(output_txt)\n",
    "        fout.write(\"\\n\")\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        # draw figures\n",
    "        _x = x_measured.numpy()\n",
    "        _y = dsdx_measured.numpy()\n",
    "        plt.plot(_x, _y, \"b*\")\n",
    "        _y1 = dsdx_calculated.detach().numpy()\n",
    "        plt.plot(_x, _y1, \"k-\")\n",
    "        os.chdir(\"records_entropy\")\n",
    "        name = str(epoch) + \".png\"\n",
    "        plt.savefig(name)\n",
    "        plt.close()\n",
    "        os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 71.5%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pybamm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
