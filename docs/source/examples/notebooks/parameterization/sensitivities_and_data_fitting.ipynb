{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybamm\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Sensitivities and data fitting using PyBaMM\n",
    "\n",
    "PyBaMM input parameters [`pybamm.InputParameter`](https://docs.pybamm.org/en/stable/source/api/expression_tree/input_parameter.html) can be used to run many simulations with varying parameters. Here we will demonstrate PyBaMM's ability to calculate the senstivities of model outputs with respect to input parameters. \n",
    "\n",
    "To be more specific, given a model output $f(a)$, where $a$ is an input parameter, we wish to calculate $\\frac{\\partial f}{\\partial a}(a)$.\n",
    "\n",
    "First we will demonstrate using a toy model, given by the equations\n",
    "\n",
    "$$\\frac{dy}{dt} = a y$$\n",
    "\n",
    "with a scalar state variable $y$ and a scalar parameter $a$, and initial conditions\n",
    "\n",
    "$$y(0) = 1$$\n",
    "\n",
    "We will also define a model output given by $f = y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a simple test model\n",
    "model = pybamm.BaseModel(\"name\")\n",
    "y = pybamm.Variable(\"y\")\n",
    "a = pybamm.InputParameter(\"a\")\n",
    "model.rhs = {y: a * y}\n",
    "model.initial_conditions = {y: 1}\n",
    "model.variables = {\"y squared\": y**2}\n",
    "\n",
    "solver = pybamm.IDAKLUSolver(rtol=1e-10, atol=1e-10)\n",
    "t_eval = np.linspace(0, 1, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Note that we have used the [`pybamm.IDAKLUSolver`](https://docs.pybamm.org/en/stable/source/api/solvers/idaklu_solver.html) solver for this example, this is currently the recommended solver for calculating sensitivities in PyBaMM.\n",
    "\n",
    "We can solve the model using a specific value of $a = 1$. However, now we will also calculate the forward sensitivities of the model by setting the argument `calculate_sensitivies=True`. Note that this argument will also accept a list of input parameters to calculate the sensitivities for, but setting it to `True` will calculate the sensitivities for **all** input parameters of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = solver.solve(model, [0, 1], inputs={\"a\": 1}, calculate_sensitivities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "We can now access the solution as normal, and the sensitivities using the syntax: `solution[output_name].sensitivities[input_parameter_name]`\n",
    "\n",
    "Note that a current restriction to the sensitivity calculation is that it will only return the sensitivities at the values of `t_eval` used to solve the model. Any interpolation between these values will have to be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(t_eval, solution[\"y squared\"](t_eval))\n",
    "axs[0].set_ylabel(r\"$y^2$\")\n",
    "axs[0].set_xlabel(r\"$t$\")\n",
    "axs[1].plot(solution.t, solution[\"y squared\"].sensitivities[\"a\"])\n",
    "axs[1].set_ylabel(r\"$\\frac{dy^2}{da}$\")\n",
    "axs[1].set_xlabel(r\"$t$\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Sensitivities for the DFN model\n",
    "\n",
    "We can do the same for the DFN model included in PyBaMM. We will setup the DFN model using \"Current function\" as an input parameter. This is the parameter we wish to calculate the sensitivities with respect to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets do the same for the DFN model\n",
    "\n",
    "# load model\n",
    "model = pybamm.lithium_ion.DFN()\n",
    "\n",
    "# load parameter values and process model and geometry\n",
    "param = model.default_parameter_values\n",
    "\n",
    "# we want to calculate the sensitivities of the \"Current function\" parameter, so set\n",
    "# this an an input parameter\n",
    "param.update({\"Current function [A]\": \"[input]\"})\n",
    "\n",
    "solver = pybamm.IDAKLUSolver(rtol=1e-3, atol=1e-6)\n",
    "\n",
    "sim = pybamm.Simulation(model, parameter_values=param, solver=solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We can now evaluate the senstivities of, for example, the \"Terminal voltage\" output of the model with respect to the input parameter \"Current function\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = sim.solve(\n",
    "    [0, 3600], inputs={\"Current function [A]\": 0.15652}, calculate_sensitivities=True\n",
    ")\n",
    "plt.plot(\n",
    "    solution.t, solution[\"Terminal voltage [V]\"].sensitivities[\"Current function [A]\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(r\"$t$\")\n",
    "plt.ylabel(\"sensitivities of Terminal voltage wrt Current fuction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Sensitivities and data fitting\n",
    "\n",
    "Sensitivities are often used to aid data fitting by providing a means to calculate the gradient of the function to be minimised. Take for example the data fitting exercise we introduced in the previous notebook. Once again we will generate some fake data for fitting, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_eval = np.linspace(0, 3600, 100)\n",
    "data = sim.solve([0, 3600], inputs={\"Current function [A]\": 0.2222})[\n",
    "    \"Terminal voltage [V]\"\n",
    "](t_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Now we will contruct a function to minimise, but here we will return both the value of the function, and its gradient with respect to the input parameter \"Current function\". Note that our objective function is the sum of squared different between the vector $\\mathbf{f}$, the simulated \"Terminal voltage\", and $\\mathbf{d}$, the vector of fake data, given by\n",
    "\n",
    "$$\\mathcal{O}(a) = \\sum_{i=0}^{i=N} (f_i(a) - d_i)^2$$ \n",
    "\n",
    "where $a$ is the parameter to be optimised (in this case \"Current function\"), $f_i$ is each element of the vector $\\mathbf{f}$, and $d_i$ is each element of $\\mathbf{d}$. We wish to also find the gradient of this function wrt the parameter $a$, which is:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{O}}{\\partial a}(a) = 2 \\sum_{i=0}^{i=N} (f_i(a) - d_i) \\frac{\\partial f_i}{\\partial a} $$ \n",
    "\n",
    "Using these equations, we will define a function that takes in as an argument $a$, and returns $(\\mathcal{O}(a), \\frac{\\partial \\mathcal{O}}{\\partial a}(a))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_jac(parameters):\n",
    "    sol = sim.solve(\n",
    "        [0, 3600],\n",
    "        t_interp=t_eval,\n",
    "        inputs={\"Current function [A]\": parameters[0]},\n",
    "        calculate_sensitivities=True,\n",
    "    )\n",
    "    term_voltage = sol[\"Terminal voltage [V]\"].data\n",
    "    term_voltage_sens = sol[\"Terminal voltage [V]\"].sensitivities[\n",
    "        \"Current function [A]\"\n",
    "    ]\n",
    "\n",
    "    f = np.sum((term_voltage - data) ** 2)\n",
    "    g = 2 * np.sum((term_voltage - data) * term_voltage_sens)\n",
    "    print(\n",
    "        f\"evaluating function and jacobian for p = {parameters[0]}, \\tloss = {f}, grad = {g}\"\n",
    "    )\n",
    "    return f, g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "We can then use this function along with an optimisation algorithm to recover the value of the Current function that was used to generate the data. In this case we will use the `scipy.optimize` module again. This module allows the use of a function in the form given above to perform the minimisation, using both the value of the objective function and its gradient to find the minimum value of $a$ in the least number of steps.\n",
    "\n",
    "Once again, we will place bounds on \"Current function [A]\" between $(0.01, 0.6)$, and use a random starting value $x_0$ between these bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = (0.01, 0.6)\n",
    "x0 = np.random.uniform(low=bounds[0], high=bounds[1])\n",
    "\n",
    "print(\"starting parameter is\", x0)\n",
    "res = scipy.optimize.minimize(sum_of_squares_jac, [x0], bounds=[bounds], jac=True)\n",
    "print(\"recovered parameter is\", res.x[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
